{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['JOBLIB_START_METHOD'] = 'forkserver'\n",
    "\n",
    "INPUT_PATH = r'E:/pg/docs/BPB/data/mercari/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "    \"\"\"\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
    "    for x in range(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x + 1])\n",
    "        for y in range(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                    and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SymSpell:\n",
    "    def __init__(self, max_edit_distance=3, verbose=0):\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.verbose = verbose\n",
    "        self.dictionary = {}\n",
    "        self.longest_word_length = 0\n",
    "\n",
    "    def get_deletes_list(self, w):\n",
    "        \"\"\"given a word, derive strings with up to max_edit_distance characters\n",
    "           deleted\"\"\"\n",
    "\n",
    "        deletes = []\n",
    "        queue = [w]\n",
    "        for d in range(self.max_edit_distance):\n",
    "            temp_queue = []\n",
    "            for word in queue:\n",
    "                if len(word) > 1:\n",
    "                    for c in range(len(word)):  # character index\n",
    "                        word_minus_c = word[:c] + word[c + 1:]\n",
    "                        if word_minus_c not in deletes:\n",
    "                            deletes.append(word_minus_c)\n",
    "                        if word_minus_c not in temp_queue:\n",
    "                            temp_queue.append(word_minus_c)\n",
    "            queue = temp_queue\n",
    "\n",
    "        return deletes\n",
    "\n",
    "    def create_dictionary_entry(self, w):\n",
    "        '''add word and its derived deletions to dictionary'''\n",
    "        # check if word is already in dictionary\n",
    "        new_real_word_added = False\n",
    "        if w in self.dictionary:\n",
    "            # increment count of word in corpus\n",
    "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
    "        else:\n",
    "            self.dictionary[w] = ([], 1)\n",
    "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
    "\n",
    "        if self.dictionary[w][1] == 1:\n",
    "            # first appearance of word in corpus\n",
    "            # n.b. word may already be in dictionary as a derived word\n",
    "            # (deleting character from a real word)\n",
    "            # but counter of frequency of word in corpus is not incremented\n",
    "            # in those cases)\n",
    "            new_real_word_added = True\n",
    "            deletes = self.get_deletes_list(w)\n",
    "            for item in deletes:\n",
    "                if item in self.dictionary:\n",
    "                    # add (correct) word to delete's suggested correction list\n",
    "                    self.dictionary[item][0].append(w)\n",
    "                else:\n",
    "                    # note frequency of word in corpus is not incremented\n",
    "                    self.dictionary[item] = ([w], 0)\n",
    "\n",
    "        return new_real_word_added\n",
    "\n",
    "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        for line in arr:\n",
    "            # separate by words by non-alphabetical characters\n",
    "            words = re.findall(token_pattern, line.lower())\n",
    "            for word in words:\n",
    "                total_word_count += 1\n",
    "                if self.create_dictionary_entry(word):\n",
    "                    unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def create_dictionary(self, fname):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        with open(fname) as file:\n",
    "            for line in file:\n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', line.lower())\n",
    "                for word in words:\n",
    "                    total_word_count += 1\n",
    "                    if self.create_dictionary_entry(word):\n",
    "                        unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def get_suggestions(self, string, silent=False):\n",
    "        \"\"\"return list of suggested corrections for potentially incorrectly\n",
    "           spelled word\"\"\"\n",
    "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
    "            if not silent:\n",
    "                print(\"no items in dictionary within maximum edit distance\")\n",
    "            return []\n",
    "\n",
    "        suggest_dict = {}\n",
    "        min_suggest_len = float('inf')\n",
    "\n",
    "        queue = [string]\n",
    "        q_dictionary = {}  # items other than string that we've checked\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            q_item = queue[0]  # pop\n",
    "            queue = queue[1:]\n",
    "\n",
    "            # early exit\n",
    "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
    "                    ((len(string) - len(q_item)) > min_suggest_len)):\n",
    "                break\n",
    "\n",
    "            # process queue item\n",
    "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
    "                if self.dictionary[q_item][1] > 0:\n",
    "                    assert len(string) >= len(q_item)\n",
    "                    suggest_dict[q_item] = (self.dictionary[q_item][1],\n",
    "                                            len(string) - len(q_item))\n",
    "                    # early exit\n",
    "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
    "                        break\n",
    "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
    "                        min_suggest_len = len(string) - len(q_item)\n",
    "\n",
    "                # the suggested corrections for q_item as stored in\n",
    "                # dictionary (whether or not q_item itself is a valid word\n",
    "                # or merely a delete) can be valid corrections\n",
    "                for sc_item in self.dictionary[q_item][0]:\n",
    "                    if sc_item not in suggest_dict:\n",
    "\n",
    "                        # compute edit distance\n",
    "                        assert len(sc_item) > len(q_item)\n",
    "\n",
    "                        # q_items that are not input should be shorter\n",
    "                        # than original string\n",
    "                        assert len(q_item) <= len(string)\n",
    "\n",
    "                        if len(q_item) == len(string):\n",
    "                            assert q_item == string\n",
    "                            item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                        # item in suggestions list should not be the same as\n",
    "                        # the string itself\n",
    "                        assert sc_item != string\n",
    "\n",
    "                        # calculate edit distance using, for example,\n",
    "                        # Damerau-Levenshtein distance\n",
    "                        item_dist = dameraulevenshtein(sc_item, string)\n",
    "\n",
    "                        # do not add words with greater edit distance if\n",
    "                        # verbose setting not on\n",
    "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
    "                            pass\n",
    "                        elif item_dist <= self.max_edit_distance:\n",
    "                            assert sc_item in self.dictionary  # should already be in dictionary if in suggestion list\n",
    "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
    "                            if item_dist < min_suggest_len:\n",
    "                                min_suggest_len = item_dist\n",
    "\n",
    "                        # depending on order words are processed, some words\n",
    "                        # with different edit distances may be entered into\n",
    "                        # suggestions; trim suggestion dictionary if verbose\n",
    "                        # setting not on\n",
    "                        if self.verbose < 2:\n",
    "                            suggest_dict = {k: v for k, v in suggest_dict.items() if v[1] <= min_suggest_len}\n",
    "\n",
    "            # now generate deletes (e.g. a substring of string or of a delete)\n",
    "            # from the queue item\n",
    "            # as additional items to check -- add to end of queue\n",
    "            assert len(string) >= len(q_item)\n",
    "\n",
    "            # do not add words with greater edit distance if verbose setting\n",
    "            # is not on\n",
    "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
    "                pass\n",
    "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
    "                for c in range(len(q_item)):  # character index\n",
    "                    word_minus_c = q_item[:c] + q_item[c + 1:]\n",
    "                    if word_minus_c not in q_dictionary:\n",
    "                        queue.append(word_minus_c)\n",
    "                        q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "\n",
    "        # queue is now empty: convert suggestions in dictionary to\n",
    "        # list for output\n",
    "        if not silent and self.verbose != 0:\n",
    "            print(\"number of possible corrections: %i\" % len(suggest_dict))\n",
    "            print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "\n",
    "        # output option 1\n",
    "        # sort results by ascending order of edit distance and descending\n",
    "        # order of frequency\n",
    "        #     and return list of suggested word corrections only:\n",
    "        # return sorted(suggest_dict, key = lambda x:\n",
    "        #               (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "        # output option 2\n",
    "        # return list of suggestions with (correction,\n",
    "        #                                  (frequency in corpus, edit distance)):\n",
    "        as_list = suggest_dict.items()\n",
    "        # outlist = sorted(as_list, key=lambda (term, (freq, dist)): (dist, -freq))\n",
    "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
    "\n",
    "        if self.verbose == 0:\n",
    "            return outlist[0]\n",
    "        else:\n",
    "            return outlist\n",
    "\n",
    "        '''\n",
    "        Option 1:\n",
    "        ['file', 'five', 'fire', 'fine', ...]\n",
    "\n",
    "        Option 2:\n",
    "        [('file', (5, 0)),\n",
    "         ('five', (67, 1)),\n",
    "         ('fire', (54, 1)),\n",
    "         ('fine', (17, 1))...]  \n",
    "        '''\n",
    "\n",
    "    def best_word(self, s, silent=False):\n",
    "        try:\n",
    "            return self.get_suggestions(s, silent)[0]\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field, start_time=time()):\n",
    "        self.field = field\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        print(f'[{time()-self.start_time}] select {self.field}')\n",
    "        dt = dataframe[self.field].dtype\n",
    "        if is_categorical_dtype(dt):\n",
    "            return dataframe[self.field].cat.codes[:, None]\n",
    "        elif is_numeric_dtype(dt):\n",
    "            return dataframe[self.field][:, None]\n",
    "        else:\n",
    "            return dataframe[self.field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DropColumnsByDf(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_df=1, max_df=1.0):\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        m = X.tocsc()\n",
    "        self.nnz_cols = ((m != 0).sum(axis=0) >= self.min_df).A1\n",
    "        if self.max_df < 1.0:\n",
    "            max_df = m.shape[0] * self.max_df\n",
    "            self.nnz_cols = self.nnz_cols & ((m != 0).sum(axis=0) <= max_df).A1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        m = X.tocsc()\n",
    "        return m[:, self.nnz_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_cat(text):\n",
    "    try:\n",
    "        cats = text.split(\"/\")\n",
    "        return cats[0], cats[1], cats[2], cats[0] + '/' + cats[1]\n",
    "    except:\n",
    "        print(\"no category\")\n",
    "        return 'other', 'other', 'other', 'other/other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brands_filling(dataset):\n",
    "    vc = dataset['brand_name'].value_counts()\n",
    "    brands = vc[vc > 0].index\n",
    "    brand_word = r\"[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\"\n",
    "\n",
    "    many_w_brands = brands[brands.str.contains(' ')]\n",
    "    one_w_brands = brands[~brands.str.contains(' ')]\n",
    "\n",
    "    ss2 = SymSpell(max_edit_distance=0)\n",
    "    ss2.create_dictionary_from_arr(many_w_brands, token_pattern=r'.+')\n",
    "\n",
    "    ss1 = SymSpell(max_edit_distance=0)\n",
    "    ss1.create_dictionary_from_arr(one_w_brands, token_pattern=r'.+')\n",
    "\n",
    "    two_words_re = re.compile(r\"(?=(\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+))\")\n",
    "\n",
    "    def find_in_str_ss2(row):\n",
    "        for doc_word in two_words_re.finditer(row):\n",
    "            print(doc_word)\n",
    "            suggestion = ss2.best_word(doc_word.group(1), silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word.group(1)\n",
    "        return ''\n",
    "\n",
    "    def find_in_list_ss1(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss1.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "\n",
    "    def find_in_list_ss2(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss2.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "\n",
    "    print(f\"Before empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "\n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_name]\n",
    "\n",
    "    n_desc = dataset[dataset['brand_name'] == '']['item_description'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_desc]\n",
    "\n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in n_name]\n",
    "\n",
    "    desc_lower = dataset[dataset['brand_name'] == '']['item_description'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in desc_lower]\n",
    "\n",
    "    print(f\"After empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "\n",
    "    del ss1, ss2\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_regex(dataset, start_time=time()):\n",
    "    karats_regex = r'(\\d)([\\s-]?)(karat|karats|carat|carats|kt)([^\\w])'\n",
    "    karats_repl = r'\\1k\\4'\n",
    "\n",
    "    unit_regex = r'(\\d+)[\\s-]([a-z]{2})(\\s)'\n",
    "    unit_repl = r'\\1\\2\\3'\n",
    "\n",
    "    dataset['name'] = dataset['name'].str.replace(karats_regex, karats_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(karats_regex, karats_repl)\n",
    "    print(f'[{time() - start_time}] Karats normalized.')\n",
    "\n",
    "    dataset['name'] = dataset['name'].str.replace(unit_regex, unit_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(unit_regex, unit_repl)\n",
    "    print(f'[{time() - start_time}] Units glued.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_pandas(train, test, start_time=time()):\n",
    "    train = train[train.price > 0.0].reset_index(drop=True)\n",
    "    print('Train shape without zero price: ', train.shape)\n",
    "\n",
    "    nrow_train = train.shape[0]\n",
    "    y_train = np.log1p(train[\"price\"])\n",
    "    merge: pd.DataFrame = pd.concat([train, test])\n",
    "\n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "\n",
    "    merge['has_category'] = (merge['category_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_category filled.')\n",
    "\n",
    "    merge['category_name'] = merge['category_name'] \\\n",
    "        .fillna('other/other/other') \\\n",
    "        .str.lower() \\\n",
    "        .astype(str)\n",
    "    merge['general_cat'], merge['subcat_1'], merge['subcat_2'], merge['gen_subcat1'] = \\\n",
    "        zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "    print(f'[{time() - start_time}] Split categories completed.')\n",
    "\n",
    "    merge['has_brand'] = (merge['brand_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_brand filled.')\n",
    "\n",
    "    merge['gencat_cond'] = merge['general_cat'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_1_cond'] = merge['subcat_1'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_2_cond'] = merge['subcat_2'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    print(f'[{time() - start_time}] Categories and item_condition_id concancenated.')\n",
    "\n",
    "    merge['name'] = merge['name'] \\\n",
    "        .fillna('') \\\n",
    "        .str.lower() \\\n",
    "        .astype(str)\n",
    "    merge['brand_name'] = merge['brand_name'] \\\n",
    "        .fillna('') \\\n",
    "        .str.lower() \\\n",
    "        .astype(str)\n",
    "    merge['item_description'] = merge['item_description'] \\\n",
    "        .fillna('') \\\n",
    "        .str.lower() \\\n",
    "        .replace(to_replace='No description yet', value='')\n",
    "    print(f'[{time() - start_time}] Missing filled.')\n",
    "\n",
    "    preprocess_regex(merge, start_time)\n",
    "\n",
    "    brands_filling(merge)\n",
    "    print(f'[{time() - start_time}] Brand name filled.')\n",
    "\n",
    "    merge['name'] = merge['name'] + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Name concancenated.')\n",
    "\n",
    "    merge['item_description'] = merge['item_description'] \\\n",
    "                                + ' ' + merge['name'] \\\n",
    "                                + ' ' + merge['subcat_1'] \\\n",
    "                                + ' ' + merge['subcat_2'] \\\n",
    "                                + ' ' + merge['general_cat'] \\\n",
    "                                + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Item description concatenated.')\n",
    "\n",
    "    merge.drop(['price', 'test_id', 'train_id'], axis=1, inplace=True)\n",
    "\n",
    "    return merge, y_train, nrow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intersect_drop_columns(train: csr_matrix, valid: csr_matrix, min_df=0):\n",
    "    t = train.tocsc()\n",
    "    v = valid.tocsc()\n",
    "    nnz_train = ((t != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_valid = ((v != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_cols = nnz_train & nnz_valid\n",
    "    res = t[:, nnz_cols], v[:, nnz_cols]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.823288202285767] Finished to load data\n",
      "Train shape:  (1482535, 8)\n",
      "Test shape:  (693359, 7)\n",
      "Train shape without zero price:  (1481661, 8)\n",
      "[8.623963117599487] Has_category filled.\n",
      "[14.868951320648193] Split categories completed.\n",
      "[15.022507905960083] Has_brand filled.\n",
      "[18.14453673362732] Categories and item_condition_id concancenated.\n",
      "[21.552385568618774] Missing filled.\n",
      "[36.10032677650452] Karats normalized.\n",
      "[52.248968839645386] Units glued.\n",
      "total words processed: 2671\n",
      "total unique words in corpus: 2671\n",
      "total items in dictionary (corpus words and deletions): 2671\n",
      "  edit distance for deletions: 0\n",
      "  length of longest word in corpus: 39\n",
      "total words processed: 2616\n",
      "total unique words in corpus: 2616\n",
      "total items in dictionary (corpus words and deletions): 2616\n",
      "  edit distance for deletions: 0\n",
      "  length of longest word in corpus: 15\n",
      "Before empty brand_name: 927861\n",
      "After empty brand_name: 252719\n",
      "[99.82573699951172] Brand name filled.\n",
      "[100.43510055541992] Name concancenated.\n",
      "[105.90042662620544] Item description concatenated.\n",
      "[106.63458204269409] select name\n",
      "[139.03702473640442] select category_name\n",
      "[150.28981137275696] select brand_name\n",
      "[156.11448431015015] select gencat_cond\n",
      "[161.96677207946777] select subcat_1_cond\n",
      "[167.755229473114] select subcat_2_cond\n",
      "[173.39508819580078] select has_brand\n",
      "[173.56991481781006] select shipping\n",
      "[173.7105073928833] select item_condition_id\n",
      "[173.8481376171112] select item_description\n",
      "[457.22079396247864] Merge vectorized\n",
      "(2175020, 8961796)\n",
      "[549.8324694633484] TF/IDF completed\n",
      "(1481661, 8961796)\n",
      "[746.3372013568878] Drop only in train or test cols: 5976503\n",
      "[906.6012871265411] Train Ridge completed. Iterations: [17]\n",
      "[907.5480546951294] Predict Ridge completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prateek1.gupta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:357: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\prateek1.gupta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #mp.set_start_method('spawn')\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    train = pd.read_table(os.path.join(INPUT_PATH, 'train.tsv'),\n",
    "                          engine='c',\n",
    "                          dtype={'item_condition_id': 'category',\n",
    "                                 'shipping': 'category'}\n",
    "                          )\n",
    "    test = pd.read_table(os.path.join(INPUT_PATH, 'test.tsv'),\n",
    "                         engine='c',\n",
    "                         dtype={'item_condition_id': 'category',\n",
    "                                'shipping': 'category'}\n",
    "                         )\n",
    "    print(f'[{time() - start_time}] Finished to load data')\n",
    "    print('Train shape: ', train.shape)\n",
    "    print('Test shape: ', test.shape)\n",
    "\n",
    "    submission: pd.DataFrame = test[['test_id']]\n",
    "\n",
    "    merge, y_train, nrow_train = preprocess_pandas(train, test, start_time)\n",
    "\n",
    "    meta_params = {'name_ngram': (1, 2),\n",
    "                   'name_max_f': 75000,\n",
    "                   'name_min_df': 10,\n",
    "\n",
    "                   'category_ngram': (2, 3),\n",
    "                   'category_token': '.+',\n",
    "                   'category_min_df': 10,\n",
    "\n",
    "                   'brand_min_df': 10,\n",
    "\n",
    "                   'desc_ngram': (1, 3),\n",
    "                   'desc_max_f': 150000,\n",
    "                   'desc_max_df': 0.5,\n",
    "                   'desc_min_df': 10}\n",
    "\n",
    "    stopwords = frozenset(['the', 'a', 'an', 'is', 'it', 'this', ])\n",
    "    # 'i', 'so', 'its', 'am', 'are'])\n",
    "\n",
    "    vectorizer = FeatureUnion([\n",
    "        ('name', Pipeline([\n",
    "            ('select', ItemSelector('name', start_time=start_time)),\n",
    "            ('transform', HashingVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                n_features=2 ** 27,\n",
    "                norm='l2',\n",
    "                lowercase=False,\n",
    "                stop_words=stopwords\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "        ])),\n",
    "        ('category_name', Pipeline([\n",
    "            ('select', ItemSelector('category_name', start_time=start_time)),\n",
    "            ('transform', HashingVectorizer(\n",
    "                ngram_range=(1, 1),\n",
    "                token_pattern='.+',\n",
    "                tokenizer=split_cat,\n",
    "                n_features=2 ** 27,\n",
    "                norm='l2',\n",
    "                lowercase=False\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "        ])),\n",
    "        ('brand_name', Pipeline([\n",
    "            ('select', ItemSelector('brand_name', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('gencat_cond', Pipeline([\n",
    "            ('select', ItemSelector('gencat_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_1_cond', Pipeline([\n",
    "            ('select', ItemSelector('subcat_1_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_2_cond', Pipeline([\n",
    "            ('select', ItemSelector('subcat_2_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('has_brand', Pipeline([\n",
    "            ('select', ItemSelector('has_brand', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('shipping', Pipeline([\n",
    "            ('select', ItemSelector('shipping', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_condition_id', Pipeline([\n",
    "            ('select', ItemSelector('item_condition_id', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_description', Pipeline([\n",
    "            ('select', ItemSelector('item_description', start_time=start_time)),\n",
    "            ('hash', HashingVectorizer(\n",
    "                ngram_range=(1, 3),\n",
    "                n_features=2 ** 27,\n",
    "                dtype=np.float32,\n",
    "                norm='l2',\n",
    "                lowercase=False,\n",
    "                stop_words=stopwords\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2)),\n",
    "        ]))\n",
    "    ], n_jobs=1)\n",
    "\n",
    "    sparse_merge = vectorizer.fit_transform(merge)\n",
    "    print(f'[{time() - start_time}] Merge vectorized')\n",
    "    print(sparse_merge.shape)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "    X = tfidf_transformer.fit_transform(sparse_merge)\n",
    "    print(f'[{time() - start_time}] TF/IDF completed')\n",
    "\n",
    "    X_train = X[:nrow_train]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    X_test = X[nrow_train:]\n",
    "    del merge\n",
    "    del sparse_merge\n",
    "    del vectorizer\n",
    "    del tfidf_transformer\n",
    "    gc.collect()\n",
    "\n",
    "    X_train, X_test = intersect_drop_columns(X_train, X_test, min_df=1)\n",
    "    print(f'[{time() - start_time}] Drop only in train or test cols: {X_train.shape[1]}')\n",
    "    gc.collect()\n",
    "\n",
    "    ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    print(f'[{time() - start_time}] Train Ridge completed. Iterations: {ridge.n_iter_}')\n",
    "\n",
    "    predsR = ridge.predict(X_test)\n",
    "    print(f'[{time() - start_time}] Predict Ridge completed.')\n",
    "\n",
    "    submission.loc[:, 'price'] = np.expm1(predsR)\n",
    "    submission.loc[submission['price'] < 0.0, 'price'] = 0.0\n",
    "    submission.to_csv(\"E:/pg/docs/BPB/data/mercari/submission_ridge.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
